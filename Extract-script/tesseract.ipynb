{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\aryan\\anaconda3\\lib\\site-packages (4.5.5.62)\n",
      "Requirement already satisfied: numpy>=1.14.5; python_version >= \"3.7\" in c:\\users\\aryan\\anaconda3\\lib\\site-packages (from opencv-python) (1.21.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\aryan\\anaconda3\\lib\\site-packages (8.0.1)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.9-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\aryan\\anaconda3\\lib\\site-packages (from pytesseract) (8.0.1)\n",
      "Collecting packaging>=21.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aryan\\anaconda3\\lib\\site-packages (from packaging>=21.3->pytesseract) (2.4.7)\n",
      "Installing collected packages: packaging, pytesseract\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\n",
      "Successfully installed packaging-21.3 pytesseract-0.3.9\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install Pillow\n",
    "!pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "F5o5Dr7o2C5J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import PIL\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image,ImageTk\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import pytesseract\n",
    "from pytesseract import Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr32MeZO2C5P",
    "outputId": "dff223f6-054c-487c-9fe0-3deb9dd37e78"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense,Flatten\n",
    "from keras.models import Sequential, load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5IPOJWj2C5Q",
    "outputId": "7a5a5097-404f-422c-8769-5c63ab04295c"
   },
   "outputs": [],
   "source": [
    "#location = input(\"Enter the location of the image : \")\n",
    "location = \"C:\\\\Users\\\\Aryan\\\\Shaan\\\\Coding\\\\04-Projects\\\\EXTRACT\\\\sentences\\\\say.png\"\n",
    "image = cv2.imread(location,0)\n",
    "thresh = cv2.threshold(image, 150, 255, cv2.THRESH_BINARY_INV)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Zp38pZiS2C5S"
   },
   "outputs": [],
   "source": [
    "result = cv2.GaussianBlur(thresh, (5,5), 0)\n",
    "result = 255 - result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FHKgkjo62C5T"
   },
   "outputs": [],
   "source": [
    "cv2.imwrite('final_img.png',result)\n",
    "loc = 'final_img.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tz5pGi3o2C5U"
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('DATA/train/.DS_Store'):\n",
    "    os.remove('DATA/train/.DS_Store')\n",
    "if os.path.isfile('DATA/test/.DS_Store'):\n",
    "    os.remove('DATA/test/.DS_Store')\n",
    "    \n",
    "for i in range(1,80):\n",
    "    if os.path.isfile(f'DATA/train/{i}/.DS_Store'):\n",
    "        os.remove(f'DATA/train/{i}/.DS_Store')\n",
    "    if os.path.isfile(f'DATA/test/{i}/.DS_Store'):\n",
    "        os.remove(f'DATA/test/{i}/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Pyt90QGo2C5U"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                            zoom_range = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "4VqT-tOu2C5V",
    "outputId": "96a9a8fd-2f49-4e96-cbfd-ffe9bd20c79b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 650 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "trained_image = datagen.flow_from_directory('DATA/train',\n",
    "                                            target_size = (32,32),\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Zw-MW1ks2C5W"
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "38ldoY_n2C5X",
    "outputId": "0ba5bed9-6b96-482c-c973-df9606af5c49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image = test_datagen.flow_from_directory('DATA/test',\n",
    "                                            target_size = (32,32),\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "diz2LPpK3soj"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-8ffc475eb9e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconv_base\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001b[0m\n\u001b[0;32m    114\u001b[0m   \"\"\"\n\u001b[0;32m    115\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;34m'The `weights` argument should be either '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;34m'`None` (random initialization), `imagenet` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), `imagenet` (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
     ]
    }
   ],
   "source": [
    "conv_base = VGG16(weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top = False, input_shape = (32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "VOqMlRV32C5X",
    "outputId": "d5b44379-464a-4328-d118-ade5d5947f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 10s 0us/step\n",
      "58900480/58889256 [==============================] - 10s 0us/step\n",
      "Epoch 1/30\n",
      "21/21 [==============================] - 20s 878ms/step - loss: 3.7652 - accuracy: 0.0323 - val_loss: 3.2394 - val_accuracy: 0.0625\n",
      "Epoch 2/30\n",
      "21/21 [==============================] - 18s 887ms/step - loss: 3.2730 - accuracy: 0.0323 - val_loss: 3.2339 - val_accuracy: 0.1250\n",
      "Epoch 3/30\n",
      "21/21 [==============================] - 19s 878ms/step - loss: 3.2840 - accuracy: 0.0262 - val_loss: 3.2577 - val_accuracy: 0.0312\n",
      "Epoch 4/30\n",
      "21/21 [==============================] - 18s 839ms/step - loss: 3.2718 - accuracy: 0.0292 - val_loss: 3.2495 - val_accuracy: 0.0625\n",
      "Epoch 5/30\n",
      "21/21 [==============================] - 17s 819ms/step - loss: 3.2455 - accuracy: 0.0477 - val_loss: 3.2648 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "21/21 [==============================] - 17s 814ms/step - loss: 3.1029 - accuracy: 0.0600 - val_loss: 2.7088 - val_accuracy: 0.0938\n",
      "Epoch 7/30\n",
      "21/21 [==============================] - 18s 856ms/step - loss: 2.8089 - accuracy: 0.0662 - val_loss: 2.4653 - val_accuracy: 0.1562\n",
      "Epoch 8/30\n",
      "21/21 [==============================] - 17s 822ms/step - loss: 2.5762 - accuracy: 0.1246 - val_loss: 2.3592 - val_accuracy: 0.2188\n",
      "Epoch 9/30\n",
      "21/21 [==============================] - 17s 826ms/step - loss: 2.3348 - accuracy: 0.1831 - val_loss: 2.2484 - val_accuracy: 0.1562\n",
      "Epoch 10/30\n",
      "21/21 [==============================] - 17s 810ms/step - loss: 2.1422 - accuracy: 0.2308 - val_loss: 2.4139 - val_accuracy: 0.3125\n",
      "Epoch 11/30\n",
      "21/21 [==============================] - 18s 837ms/step - loss: 2.0850 - accuracy: 0.2431 - val_loss: 1.7458 - val_accuracy: 0.3125\n",
      "Epoch 12/30\n",
      "21/21 [==============================] - 17s 813ms/step - loss: 1.6607 - accuracy: 0.3477 - val_loss: 1.6380 - val_accuracy: 0.3438\n",
      "Epoch 13/30\n",
      "21/21 [==============================] - 20s 935ms/step - loss: 1.4718 - accuracy: 0.4400 - val_loss: 1.2080 - val_accuracy: 0.4688\n",
      "Epoch 14/30\n",
      "21/21 [==============================] - 18s 870ms/step - loss: 1.2618 - accuracy: 0.5292 - val_loss: 1.1690 - val_accuracy: 0.5312\n",
      "Epoch 15/30\n",
      "21/21 [==============================] - 19s 893ms/step - loss: 1.1412 - accuracy: 0.5538 - val_loss: 0.8278 - val_accuracy: 0.6250\n",
      "Epoch 16/30\n",
      "21/21 [==============================] - 17s 799ms/step - loss: 0.9301 - accuracy: 0.5985 - val_loss: 1.0301 - val_accuracy: 0.5938\n",
      "Epoch 17/30\n",
      "21/21 [==============================] - 19s 896ms/step - loss: 1.2935 - accuracy: 0.5400 - val_loss: 1.1589 - val_accuracy: 0.6250\n",
      "Epoch 18/30\n",
      "21/21 [==============================] - 17s 807ms/step - loss: 1.0338 - accuracy: 0.6000 - val_loss: 0.6728 - val_accuracy: 0.7812\n",
      "Epoch 19/30\n",
      "21/21 [==============================] - 17s 829ms/step - loss: 0.8706 - accuracy: 0.6446 - val_loss: 0.8615 - val_accuracy: 0.7188\n",
      "Epoch 20/30\n",
      "21/21 [==============================] - 18s 870ms/step - loss: 0.7822 - accuracy: 0.7000 - val_loss: 0.7130 - val_accuracy: 0.7188\n",
      "Epoch 21/30\n",
      "21/21 [==============================] - 19s 895ms/step - loss: 0.5407 - accuracy: 0.7754 - val_loss: 0.3435 - val_accuracy: 0.8125\n",
      "Epoch 22/30\n",
      "21/21 [==============================] - 17s 822ms/step - loss: 0.4995 - accuracy: 0.8246 - val_loss: 0.4195 - val_accuracy: 0.7812\n",
      "Epoch 23/30\n",
      "21/21 [==============================] - 18s 843ms/step - loss: 0.4181 - accuracy: 0.8354 - val_loss: 0.4513 - val_accuracy: 0.8125\n",
      "Epoch 24/30\n",
      "21/21 [==============================] - 17s 803ms/step - loss: 0.4102 - accuracy: 0.8508 - val_loss: 0.2162 - val_accuracy: 0.9688\n",
      "Epoch 25/30\n",
      "21/21 [==============================] - 18s 843ms/step - loss: 0.5906 - accuracy: 0.8015 - val_loss: 0.9647 - val_accuracy: 0.6875\n",
      "Epoch 26/30\n",
      "21/21 [==============================] - 17s 827ms/step - loss: 0.4754 - accuracy: 0.8077 - val_loss: 0.2409 - val_accuracy: 0.8750\n",
      "Epoch 27/30\n",
      "21/21 [==============================] - 19s 884ms/step - loss: 0.4269 - accuracy: 0.8631 - val_loss: 0.3661 - val_accuracy: 0.9062\n",
      "Epoch 28/30\n",
      " 8/21 [==========>...................] - ETA: 11s - loss: 0.6401 - accuracy: 0.8376"
     ]
    }
   ],
   "source": [
    "def Train_Model():\n",
    "    global trained_image, test_image\n",
    "    model = Sequential()\n",
    "    conv_base = VGG16(weights='imagenet', include_top = False, input_shape = (32,32,3))\n",
    "    model.add(conv_base)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(26,activation = 'softmax'))\n",
    "    model.compile(optimizer= 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    model.fit(trained_image, epochs = 30, validation_data = test_image, validation_steps = 1)\n",
    "    model.save('model.h5')\n",
    "    \n",
    "Train_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pPC8sgnL2C5Y"
   },
   "outputs": [],
   "source": [
    "def Predict_Model(img):\n",
    "    global trained_image\n",
    "    new_model = load_model('model.h5')\n",
    "    img = cv2.resize(img,(32,32),3)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = img / 255\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z','A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','1','2','3','4','5','6','7','8','9','0',',',';',':','?','!','.','@','#','$','%','&','(',')','{','}','[',']']\n",
    "    \n",
    "    #prediction = new_model.predict_classes(img)\n",
    "    predict_x=new_model.predict(img) \n",
    "    \n",
    "    classes_x=np.argmax(predict_x,axis=1)\n",
    "    \n",
    "    prediction = classes_x[0]\n",
    "    \n",
    "    my_dict = dict(trained_image.class_indices)\n",
    "    \n",
    "    for key,value in my_dict.items():\n",
    "        if prediction == value:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZC13RDoE2C5Z"
   },
   "outputs": [],
   "source": [
    "def Word_Extract(location):\n",
    "    \n",
    "    if os.path.isdir('WORDS'):\n",
    "        shutil.rmtree('WORDS')\n",
    "        os.mkdir('WORDS')\n",
    "    else:\n",
    "        os.mkdir('WORDS')\n",
    "    img = PIL.Image.open(location)\n",
    "    d = pytesseract.image_to_data(Image.open(location) , output_type=Output.DICT)\n",
    "    n_boxes = len(d['level'])\n",
    "    select =[]\n",
    "    j =0\n",
    "    for i in range(n_boxes):\n",
    "        if d['text'][i] != '' and d['conf'][i] != '-1':\n",
    "            select.append(d['text'][i])\n",
    "            (x, y, w, h) = (d['left'][i]-20, d['top'][i]-20, d['width'][i]+20, d['height'][i]+20)\n",
    "            #cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            crop_img = img.crop((x,y,x+w,y+h))\n",
    "            crop_img.save(f'WORDS/{j}.png')\n",
    "            j += 1\n",
    "    \n",
    "    return  select\n",
    "      \n",
    "    \n",
    "select = Word_Extract(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Iy3jTM6U2C5Z"
   },
   "outputs": [],
   "source": [
    "if os.path.isdir('LETTERS'):\n",
    "    shutil.rmtree('LETTERS')\n",
    "    os.mkdir('LETTERS')\n",
    "else:\n",
    "    os.mkdir('LETTERS')\n",
    "    \n",
    "    \n",
    "for i in range(len(os.listdir('WORDS'))):\n",
    "    os.mkdir(f'LETTERS/{i}')\n",
    "    img = Image.open(f\"WORDS/{i}.png\")\n",
    "    img = img.resize((1600 , 800))\n",
    "    w,h = img.size\n",
    "    letters = pytesseract.image_to_boxes(img , output_type=Output.DICT)\n",
    "    letters\n",
    "    \n",
    "    idx = 0 \n",
    "    for c in range(len(letters['char'])): \n",
    "        \n",
    "        (x, y, w, h) = (letters['left'][c], letters['bottom'][c], letters['right'][c], letters['top'][c])\n",
    "        \n",
    "        crop_img = img.crop((x-50,y-50 , w+50,h+50))\n",
    "        crop_img.save(f\"LETTERS/{i}/\" + str(idx) + '.png')\n",
    "        idx+=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3Fx-5j4n2C5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015BBC10B160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000015BBC10BDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for i in range(len(os.listdir('LETTERS'))):\n",
    "    string = ''\n",
    "    char_dict = {}\n",
    "    for j in range(len(os.listdir(f'LETTERS/{i}'))):\n",
    "        img = cv2.imread(f'LETTERS/{i}/{j}.png')\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        char = Predict_Model(img)\n",
    "        \n",
    "        \n",
    "        for l in range(len(select[i])):\n",
    "            if char == select[i][l]:\n",
    "                char_dict[select[i].index(char)] = char\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                for k in range(len(select[i])):\n",
    "                    if k != l:\n",
    "                        if char == select[i][k]:\n",
    "                            char_dict[k] = char \n",
    "        \n",
    "        keys = list(char_dict.keys())\n",
    "        if char not in select[i]:\n",
    "            for val in range(len(select[i])):\n",
    "                if val not in keys:\n",
    "                    char_dict[val] = char\n",
    "    keys = list(char_dict.keys())\n",
    "    keys.sort()\n",
    "    #print(keys)\n",
    "    \n",
    "    for m in keys:\n",
    "        string += char_dict[m]\n",
    "        \n",
    "        \n",
    "    string += ' '\n",
    "    text += string\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "AjpZgjRr2C5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The predicted sentence is:  AKARSH MALIK \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"\")\n",
    "print(\"The predicted sentence is: \",text)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tesseract.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
